##
## Compare social relations models
## 
## The 'amen' package doesn't do model selection, as such, so we take two appoaches here:
##
## 1. Use 'amen's goodness of fit function to calculate root mean square errors (RMSE) between each model
##    and the observed networks. Smallest RMSE overall = best model
##
## 2. Calculate F_1 scores and Accuracy metrics for each of the models' predictions compared to observed data
##
## Make sure you run 'social relations model - fit.r' first
##
library(ggplot2)

source("init if necessary.r")

flog.info("Comparing social relations models")

load(file.path(models.dir, "social relations models.rdata"))

model_names = c("Intercept-only", "r + siida + gift", "r x siida + gift", "r x siida x gift")


#######################################################################
## Goodness of fit
## which model had the smallest discrepancies between the posterior predictive distributions and the observed statistics?
##
flog.info("... goodness of fit")

#' Function to calculate the root mean square errors, mean absolute errors and median absolute errors
#' for each of the four goodness of fit stats generated by 'amen'
#' (though we only use RMSE here)
#' 
#' @param Y, adjacency matrix for observed network ties
#' @param srrm, the SRM
#' @return A named list containing RMSEs, MAEs and MedianAEs for each of the four goodness of fit stats
#' 
gof_diffs = function(Y, srrm)
{
  ## debug ###
  # Y = adj.coop
  # srrm = fit_srrm3
  ###
  
  GOF = gofstats(Y)  # empirical standard deviations
  
  n = length(srrm$GOF[-1, 1])  # no. sims in posterior distribution
  
  # calculate the root mean square error between the posterior predictive distribution and the observed stats
  # for the SD of ego means, SD of alter means and the within-dyad correlation
  rmse = c()
  mdae = c()  # median absolute error
  mae = c()   # mean absolute error
  for (k in 1:4)
  {
    rmse[k]  = sqrt( sum( (srrm$GOF[-1, k] - GOF[k])^2 ) / n )
    mae[k] = mean(abs(srrm$GOF[-1, k] - GOF[k]))
    mdae[k] = median(abs(srrm$GOF[-1, k] - GOF[k]))
  }
  # return(rmse)
  return(list(
    rmse=rmse,
    mae=mae,
    median_ae=mdae
  ))
}

# calculate goodness of fit discrepancies for each of our models
gof_null  = gof_diffs(adj.coop, fit_null_ind)
gof_srrm2 = gof_diffs(adj.coop, fit_srrm2)
gof_srrm3 = gof_diffs(adj.coop, fit_srrm3)
gof_srrm4 = gof_diffs(adj.coop, fit_srrm4)

# find which models had lowest RMSEs for each of the four goodness of fit stats
for (i in 1:4)
{
  # print("RMSE:")
  print( model_names[
    which.min( c(gof_null$rmse[i], gof_srrm2$rmse[i], gof_srrm3$rmse[i], gof_srrm4$rmse[i]) )
    ] )

  # print("\nMAE")
  # print( model_names[
  #   which.min( c(gof_null$mae[i], gof_srrm2$mae[i], gof_srrm3$mae[i], gof_srrm4$mae[i]) )
  #   ] )
  
  # print("\nMedian AE")
  # print( model_names[
  #   which.min( c(gof_null$median_ae[i], gof_srrm2$median_ae[i], gof_srrm3$median_ae[i], gof_srrm4$median_ae[i]) )
  #   ] )
}

# construct and save table of root mean square errors
rmsd = as.data.frame(matrix(c(
  (gof_null$rmse),
  (gof_srrm2$rmse),
  (gof_srrm3$rmse),
  (gof_srrm4$rmse)
), ncol=4, nrow=4, byrow=T))

names(rmsd) = c("SD ego means",	"SD alter means",	"Dyadic correlations", "Triadic dependence")
rmsd$Model = model_names
rmsd = rmsd %>% select(Model, everything())  # put Model column first

write_csv(rmsd, file.path(results.dir, "social relations model - root mean square errors.csv"))


#######################################################################
## Simulate models' predictions of observed data and calculate performance using F_1 score and Accuracy
##
flog.info("... accuracy and F1 scores")

#' Calculate Accuracy and F1 scores for a social relations model by simulating from the posteriors.
#'
#' @param srrm The social relations model
#' @param link_fun Link function to use for this model
#'
#' @return Named list containing F1 scores and Accuracies
#' 
calc_metrics = function(srrm, link_fun)
{
  # srrm = fit_srrm3
  # link_fun = p.link_srrm3

  # get posterior mean estimates of ego and alter random effects for each observation
  a_i  = srrm$APM
  b_j  = srrm$BPM
  re = a_i[ as.character(herders.wide$Ego) ] + b_j[ as.character(herders.wide$Alter) ]

  # helper function to vectorise the adjacency matrices (and remove self-loops)
  mat_vec = function(adj) {
    diag(adj) = NA
    d = as.vector(adj[ as.character(herders$HerderID), as.character(herders$HerderID) ])
    d = na.omit(d)
    d
  }
  
  siida = mat_vec(adj.siida)
  r     = mat_vec(adj.kin)
  gift  = mat_vec(adj.gifts)
  coop  = mat_vec(adj.coop)
  
  # calculate model predictions
  # pred.raw = sapply(1:nrow(herders.wide), function(i) link_fun(srrm, herders.wide$r[i], herders.wide$SameSiida[i], herders.wide$GiftGiven[i], re[i]))
  pred.raw = sapply(1:nrow(herders.wide), function(i) link_fun(srrm, r[i], siida[i], gift[i], re[i]))
  #str(pred.raw)
  
  # simulate posterior predictoins
  # - one column for each observed dyad
  # - one row for each simulation
  post_pred = sapply(1:ncol(pred.raw), function(s) {
    rbinom(nrow(pred.raw), size = 1, prob = pred.raw[, s])
  })
  
  # for each column (simulated posterior prediction), was it correct?
  true_pos = c(); true_neg = c(); false_pos = c(); false_neg = c()
  for (i in 1:nrow(post_pred))
  {
    true_pos[i]  = sum(coop==1 & post_pred[i,]==1)
    true_neg[i]  = sum(coop==0 & post_pred[i,]==0)
    false_pos[i] = sum(coop==0 & post_pred[i,]==1)
    false_neg[i] = sum(coop==1 & post_pred[i,]==0)
  }

  # calc precision and recall
  precision = true_pos / (true_pos + false_pos)
  recall = true_pos / (true_pos + false_neg)
  
  # F_1 score
  f1 = (2 * precision * recall) / (precision + recall)
  
  # accuracy
  accuracy = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)

  return(list(
    f1 = f1,
    accuracy = accuracy
  ))
}

# calculate metrics
metrics_null  = calc_metrics(fit_null_ind, p.link_null)
metrics_srrm2 = calc_metrics(fit_srrm2, p.link_srrm2)
metrics_srrm3 = calc_metrics(fit_srrm3, p.link_srrm3)
metrics_srrm4 = calc_metrics(fit_srrm4, p.link_srrm4)

# put everything into one table for plotting
metrics = data.frame(
  Model = rep(model_names, each=length(metrics_srrm2$f1)),
  ModelNum = rep(1:4, each=length(metrics_srrm2$f1)),
  F1 = c(metrics_null$f1, metrics_srrm2$f1, metrics_srrm3$f1, metrics_srrm4$f1),
  Accuracy = c(metrics_null$accuracy, metrics_srrm2$accuracy, metrics_srrm3$accuracy, metrics_srrm4$accuracy)
)

# plot F1 scores and accuracies, and save into single image
plt_f1 = ggplot(metrics, aes(x=factor(ModelNum), y=F1)) + 
  geom_boxplot() +
  xlab("Social relations model") +
  ylab("F1 scores") +
  ggtitle("a") +
  common_theme

plt_acc = ggplot(metrics, aes(x=factor(ModelNum), y=Accuracy)) + 
  geom_boxplot() +
  xlab("Social relations model") +
  ylab("Accuracy") +
  ggtitle("b") +
  common_theme

png(file.path(plots.dir, "SRM - metrics.png"), height=10, width=20, units="cm", res=300)
  gridExtra::grid.arrange(plt_f1, plt_acc, ncol=2)
dev.off()

pdf(file.path(plots.dir, "SRM - metrics.pdf"), height=5, width=10)
  gridExtra::grid.arrange(plt_f1, plt_acc, ncol=2)
dev.off()

# done
save(model_names, gof_diffs, gof_null, gof_srrm2, gof_srrm3, gof_srrm4, rmsd,
     calc_metrics, metrics, metrics_null, metrics_srrm2, metrics_srrm3, metrics_srrm4,
     file=file.path(models.dir, "social relations models - comparison.rdata"))

flog.info("Finished comparing")
